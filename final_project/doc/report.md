# å®éªŒæŠ¥å‘Š
åŸºäºæ·±åº¦å­¦ä¹ çš„ä¸­è‹±æœºå™¨ç¿»è¯‘  
17341046 éƒ­æ¢“ç…œ
<!-- TOC -->

- [å®éªŒæŠ¥å‘Š](#å®éªŒæŠ¥å‘Š)
    - [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
    - [å®éªŒè¦æ±‚](#å®éªŒè¦æ±‚)
    - [å®éªŒè¿‡ç¨‹](#å®éªŒè¿‡ç¨‹)
        - [æ•°æ®é¢„å¤„ç†](#æ•°æ®é¢„å¤„ç†)
        - [æ¨¡å‹](#æ¨¡å‹)
            - [Encoder](#encoder)
            - [Decoder](#decoder)
            - [Attention Module](#attention-module)
        - [è®­ç»ƒ](#è®­ç»ƒ)
        - [æµ‹è¯•](#æµ‹è¯•)
    - [å®éªŒç»“æœ](#å®éªŒç»“æœ)
    - [æ€»ç»“ä¸æ€è€ƒ](#æ€»ç»“ä¸æ€è€ƒ)
        - [é‡åˆ°çš„å›°éš¾åŠè§£å†³æ–¹æ³•](#é‡åˆ°çš„å›°éš¾åŠè§£å†³æ–¹æ³•)
        - [å¿ƒå¾—ä½“ä¼š](#å¿ƒå¾—ä½“ä¼š)

<!-- /TOC -->
## ç¯å¢ƒé…ç½®
- python3.6
- vscode
- nltkï¼Œjieba
- TensorFlow1.3.0ï¼ŒKeras
- coding: utf-8

## å®éªŒè¦æ±‚
1. æ¨¡å‹è¦æ±‚ï¼š  
   - ä¸¤ä¸ªLSTMåˆ†åˆ«ä½œä¸ºEncoderå’ŒDecoder
   - å®ç°åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æœºå™¨ç¿»è¯‘
   - è‡ªè¡Œé€‰æ‹©åˆ†è¯å·¥å…·
   - æ”¹å˜teacher forcing ratioï¼Œè§‚å¯Ÿæ•ˆæœ
   - Beam Searchç­–ç•¥
2. è¯„ä¼°æŒ‡æ ‡ï¼šBLEUå€¼ï¼ˆBLEU-4ï¼‰
3. è¯å‘é‡ï¼šéšæœºåˆå§‹åŒ–æˆ–è‡ªé€‰é¢„è®­ç»ƒè¯å‘é‡
4. è®¾å¤‡ï¼šCPU/GPU
5. æ•°æ®é›†è§„æ¨¡ï¼š10000  
10Kç‰ˆæœ¬ï¼Œtrain: 8000, test: 1000, dev: 1000  
100Kç‰ˆæœ¬ï¼ˆéå¿…é¡»ï¼‰ï¼Œtrain: 80000, test: 10000, dev: 10000  
å¯æ ¹æ®æä¾›çš„æ•°æ®å¤„ç†è„šæœ¬è‡ªå®šä¹‰æ•°æ®é›†å¤§å°ï¼ˆä¸èƒ½å¤ªå°ï¼‰  
æ•°æ®æ ¼å¼ï¼š			 
source: æ¯è¡Œä¸€æ¡ä¸­æ–‡å¥å­  
target: æ¯è¡Œä¸€æ¡sourceä¸­å¯¹åº”è¡Œæ•°çš„è‹±æ–‡å¥å­  

## å®éªŒè¿‡ç¨‹
### æ•°æ®é¢„å¤„ç†
1. å®šä¹‰ç‰¹æ®Šç¬¦å·  
å®šä¹‰ä¸€äº›ç‰¹æ®Šç¬¦å·ã€‚å…¶ä¸­â€œ\<pad>â€åŠ åœ¨è¾ƒçŸ­åºåˆ—åï¼Œç›´åˆ°åŒä¸€batchå†…æ¯ä¸ªæ ·æœ¬åºåˆ—ç­‰é•¿ã€‚è€Œâ€œ\<bos>â€å’Œâ€œ\<eos>â€ç¬¦å·åˆ†åˆ«è¡¨ç¤ºåºåˆ—çš„å¼€å§‹å’Œç»“æŸï¼Œè¦æ±‚æ¯ä¸ªå¥å­å¼€å¤´ä¸ºâ€œ\<bos>â€ï¼Œç»“å°¾ä¸ºâ€œ\<eos>â€
2. åˆ†è¯  
å¯¹è¯­æ–™é›†å†…çš„å¥å­è¿›è¡Œåˆ†è¯ï¼Œå¯ä»¥é€‰æ‹©æ ¹æ®ç©ºæ ¼åˆ†è¯æˆ–è€…ä½¿ç”¨Spacyï¼ŒNLTKç­‰å·¥å…·è¿›è¡Œåˆ†è¯
3. åˆ›å»ºè¯å…¸  
æ ¹æ®ä¸Šè¿°åˆ†è¯ç»“æœåˆ†åˆ«ä¸ºæºè¯­è¨€å’Œç›®æ ‡è¯­è¨€åˆ›å»ºè¯å…¸ï¼›æºè¯­è¨€å•è¯çš„ç´¢å¼•å’Œç›®æ ‡è¯­è¨€å•è¯çš„ç´¢å¼•ç›¸äº’ç‹¬ç«‹

- æ ¹æ®æ–‡ä»¶è·¯å¾„ï¼Œè¯»å–æ–‡æ¡£
  - ä½¿ç”¨nltkï¼Œjiebaåˆ†è¯
  - åœ¨å¥å­å¼€å¤´æ’å…¥\<bos>ï¼Œç»“å°¾æ’å…¥\<bos>
    ```py
    def get_target_sentense(file_path):

    target_sentense = []
    f = open(file_path, 'r', encoding='utf-8')
    for sentense in f.readlines():
        sentense = sentense.strip()
        if len(sentense) == 0:
            continue
        temp = nltk.word_tokenize(sentense.lower()) 
        # ç”¨nltkè¿›è¡Œåˆ†è¯ï¼Œå•è¯è½¬æ¢æˆå°å†™
        temp.insert(0, '<bos>')  # å¼€å¤´æ’å…¥<bos>
        temp.append('<eos>')  # ç»“å°¾æ’å…¥<eos>
        target_sentense.append(temp)
    return target_sentense
    ```
    ```py
    def get_score_sentense(file_path):

    score_sentense = []
    f = open(file_path, 'r', encoding='utf-8')
    for sentense in f.readlines():
        sentense = sentense.strip()
        word_list = jieba.cut(sentense, cut_all=False)  # ç”¨jiebaåˆ†è¯
        sentense = ' '.join(word_list)
        temp_list = sentense.split()
        temp_list.insert(0, '<bos>')  # åœ¨å¥å­å¼€å¤´æ’å…¥<bos>
        temp_list.append('<eos>')  # åœ¨å¥å­ç»“å°¾æ’å…¥<eos>
        score_sentense.append(temp_list)
    return score_sentense
    ```
- åˆ›å»ºè¯å…¸
  - å°†å•è¯è½¬ä¸ºæ•°å­—
    ```py
    def word2num(word_to_num, sentense_list):

    num_sentense = []
    for sentense in sentense_list:
        num_sentense = []
        for word in sentense:
            vocab_num = word_to_num[word]
            num_sentense += [vocab_num]
        num_sentense += [num_sentense]
    return num_sentense
    ```
  - æ’åºå¾—åˆ°å­—å…¸
    ```py
    def build_dict(data):

    counter = collections.Counter(data)
    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))
    words, _ = list(zip(*count_pairs))
    word_to_num = dict(zip(words, range(len(words)))) 
    return word_to_num
    ```
### æ¨¡å‹
#### Encoder  
1. æ ¹æ®æºè¯­è¨€è¯å…¸å¤§å°è®¾ç½®word embeddingçŸ©é˜µï¼›ç”¨é¢„è®­ç»ƒè¯å‘é‡åˆå§‹åŒ–  
    ```py
    def _init_embedding(self):
        self.embedding_encoder = tf.Variable(tf.random_uniform(
            [self.source_vocab_size,
             self.embedding_size]))
        self.encoder_embedding_inputs = tf.nn.embedding_lookup(
            self.embedding_encoder,
            self.encoder_inputs)

        self.embedding_decoder = tf.Variable(tf.random_uniform(
            [self.target_vocab_size,
             self.embedding_size]))

        self.decoder_embedding_inputs = tf.nn.embedding_lookup(
            self.embedding_decoder,
            self.decoder_targets)
    ```
2. Encoderä½¿ç”¨åŒå‘LSTM;
3. Encoderçš„åˆå§‹éšè—çŠ¶æ€h0é€‰æ‹©å…¨é›¶æˆ–è€…éšæœºå‘é‡ï¼›æºå¥å­çš„æ¯ä¸ªå•è¯çš„embeddingä½œä¸ºEncoderçš„ç›¸åº”æ—¶é—´æ­¥è¾“å…¥;
4. Encoderè¿”å›outputå‘é‡ï¼Œå…¶ç»´åº¦å¤§å°ä¸º [src_legth, batch_size, hid_dim*num_directions];  
è¿™é‡Œå¯ä»¥å°†ï¼ˆhid_dim*num_directionsï¼‰çœ‹æˆæ˜¯å‰å‘ã€åå‘éšè—çŠ¶æ€çš„æ‹¼æ¥; è¯¥å‘é‡çš„ç¬¬ä¸€ç»´ä¸­ç¬¬iä¸ªåˆ†é‡ä½œä¸ºæ¯ä¸ªbatchä¸‹æºå¥å­çš„ç¬¬iæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚

    ```py
    def _init_encoder(self):
        def make_cell(rnn_size):
            enc_cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)
            return enc_cell

        with tf.variable_scope("Encoder") as scope:
            num_layers = self.num_layers
            encoder_cell_fw = make_cell(self.encoder_num_units)
            encoder_cell_bw = make_cell(self.encoder_num_units)
            #state æ˜¯æ¯ä¸€å±‚æœ€åä¸€ä¸ªstepçš„è¾“å‡º
            enc_outputs, enc_state = tf.nn.bidirectional_dynamic_rnn(
                encoder_cell_fw,
                encoder_cell_bw,
                inputs=self.encoder_embedding_inputs,
                sequence_length=self.encoder_inputs_length,
                dtype=tf.float32,
            )
            self.encoder_outputs = tf.concat([enc_outputs[0],enc_outputs[1]], -1)
            self.encoder_state = enc_state
    ```
   
#### Decoder  
1. Decoderä½¿ç”¨å•å‘LSTMæˆ–è€…å•å‘GRUï¼Œæ ¹æ®ç›®æ ‡è¯­è¨€è¯å…¸å¤§å°è®¾ç½®word embeddingçŸ©é˜µğ‘Š_i  ï¼Œä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡åˆå§‹åŒ–;

2. ç”±äºå¸Œæœ›Decoderèƒ½å°½é‡è·å–encoderçš„ç¼–ç ä¿¡æ¯ï¼Œæ‰€ä»¥é€‰æ‹©Encoderçš„æœ€åä¸€ä¸ªéšè—çŠ¶æ€ï¼ˆåŒå‘ï¼‰htä½œä¸ºDecoderçš„åˆå§‹éšè—çŠ¶æ€s0ï¼›
3. è®­ç»ƒæ—¶å€™ï¼ŒDecoderçš„è¾“å…¥æœ‰å¦‚ä¸‹ä¸¤ç§æ–¹å¼ï¼š  
       a. Teacher Forcing:ç›´æ¥ä½¿ç”¨è®­ç»ƒæ•°æ®çš„æ ‡å‡†ç­”æ¡ˆ(ground truth)çš„å¯¹åº”ä¸Šä¸€é¡¹ä½œä¸ºå½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ï¼›  
       b. Curriculum Learning:ä½¿ç”¨ä¸€ä¸ªæ¦‚ç‡p,éšæœºå†³å®šé€‰æ‹©ä½¿ç”¨ground truthè¿˜æ˜¯å‰ä¸€ä¸ªæ—¶é—´æ­¥æ¨¡å‹ç”Ÿæˆçš„é¢„æµ‹ï¼Œæ¥ä½œä¸ºå½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ã€‚
4.ä¸ä½¿ç”¨attentionæœºåˆ¶çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ç›´æ¥å°†RNNæ¯ä¸ªæ—¶é—´æ­¥ä¸‹çš„éšè—çŠ¶æ€htï¼Œç»è¿‡å…¨è¿æ¥å±‚åè¾“å‡º  
$ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡=ğ‘Š_ğ‘œ â„_ğ‘¡$  
 $ğ‘Š_ğ‘œ$ï¼šå…¨è¿æ¥å±‚ï¼Œå°†æ¯ä¸ªæ—¶é—´æ­¥ä¸‹çš„éšè—çŠ¶æ€" è½¬åŒ–"ä¸ºç»´åº¦ V çš„" è¾“å‡ºå‘é‡logitï¼ŒVæ˜¯ç›®æ ‡è¯­è¨€çš„è¯å…¸å¤§å°ï¼›è¾“å‡ºå‘é‡logitå¯çœ‹ä½œä¸ºæœ‰å…³å„ä¸ªè¾“å‡ºè¯çš„é¢„æµ‹æ¦‚ç‡ï¼Œç»´åº¦ä¸º(batch_size, V)ï¼›è¿™é‡Œçš„$ğ‘Š_ğ‘œ$å¯ä»¥è®¾ç½®ä¸º$ğ‘Š_i$  çš„è½¬ç½®çŸ©é˜µï¼Œæˆ–è€…ç‹¬ç«‹çš„çŸ©é˜µã€‚

    ```py
    def _init_decoder(self):
        def make_cell(rnn_size):
            dec_cell = tf.nn.rnn_cell.BasicLSTMCell(rnn_size)
            return dec_cell

        def create_decoder_cell():
            cell = tf.contrib.rnn.MultiRNNCell([make_cell(self.decoder_num_units) for _ in range(self.num_layers)])

            if self.beam_search and self.mode == "Infer": 
                dec_start_state = seq2seq.tile_batch(self.encoder_state, self.beam_width) 
                #å°†encoder_stateå¤åˆ¶beam_widthä»½
                enc_outputs = seq2seq.tile_batch(self.encoder_outputs, self.beam_width)   
                #å°†encoder_outputså¤åˆ¶beam_widthä»½
                enc_lengths = seq2seq.tile_batch(self.encoder_inputs_length, self.beam_width) 
                #å°†encoder_inputs_lengthå¤åˆ¶beam_widthä»½
            else:
                dec_start_state = self.encoder_state #encoderæœ€åä¸€ä¸ªéšè—çŠ¶æ€ä½œä¸ºdecoderçš„åˆå§‹çŠ¶æ€
                enc_outputs = self.encoder_outputs
                enc_lengths = self.encoder_inputs_length

            if self.attention:
                attention_states = enc_outputs

                attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                    self.decoder_num_units,
                    attention_states,
                    memory_sequence_length=enc_lengths)

                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                    cell,
                    attention_mechanism,
                    attention_layer_size=self.decoder_num_units)

                if self.beam_search and self.mode == "Infer":
                    initial_state = decoder_cell.zero_state(self.batch_size * self.beam_width, tf.float32)
                else:
                    initial_state = decoder_cell.zero_state(self.batch_size, tf.float32)

                initial_state = initial_state.clone(cell_state=dec_start_state)
            else:
                initial_state = dec_start_state

            return decoder_cell, initial_state

        with tf.variable_scope("Decoder") as scope:
            projection_layer = layers_core.Dense(units=self.target_vocab_size, use_bias=False)  

            decoder_cell, initial_state = create_decoder_cell()

            if self.mode == "Train":
                training_helper = tf.contrib.seq2seq.TrainingHelper(
                    self.decoder_embedding_inputs,
                    self.decoder_train_length)

                training_decoder = tf.contrib.seq2seq.BasicDecoder(
                    cell=decoder_cell,
                    helper=training_helper,
                    initial_state=initial_state,
                    output_layer=projection_layer)

                (self.decoder_outputs_train,
                 self.decoder_state_train,
                 final_sequence_length) = tf.contrib.seq2seq.dynamic_decode(
                    decoder=training_decoder,
                    impute_finished=True,
                    scope=scope
                )

                self.decoder_logits_train = self.decoder_outputs_train.rnn_output
                decoder_predictions_train = tf.argmax(self.decoder_logits_train, axis=-1)
                self.decoder_predictions_train = tf.identity(decoder_predictions_train)

            elif self.mode == "Infer":
                start_tokens = tf.tile(tf.constant([2], dtype=tf.int32), [self.batch_size])

                if self.beam_search == True:
                    inference_decoder = tf.contrib.seq2seq.BeamSearchDecoder(
                        cell=decoder_cell,
                        embedding=self.embedding_decoder,
                        start_tokens=tf.ones_like(self.encoder_inputs_length) * tf.constant(2, dtype=tf.int32), #decoderç¬¬ä¸€ä¸ªç¼–ç å™¨çš„è¾“å…¥
                        #start_tokens = start_tokens,
                        end_token=tf.constant(0, dtype=tf.int32), #ç»ˆæ­¢ç¬¦
                        initial_state=initial_state,
                        beam_width=self.beam_width,
                        output_layer=projection_layer)

                    self.decoder_outputs_inference, __, ___ = tf.contrib.seq2seq.dynamic_decode(
                        decoder=inference_decoder,
                        maximum_iterations=tf.round(tf.reduce_max(self.encoder_inputs_length)),
                        #maximum_iterations = 30,
                        impute_finished=False,
                        scope=scope)

                    self.decoder_predictions_inference = tf.identity(self.decoder_outputs_inference.predicted_ids)

                else:
                    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                        self.embedding_decoder,
                        start_tokens=start_tokens,
                        end_token=0)  # EOS id

                    inference_decoder = tf.contrib.seq2seq.BasicDecoder(
                        cell=decoder_cell,
                        helper=inference_helper,
                        initial_state=initial_state,
                        output_layer=projection_layer)

                    self.decoder_outputs_inference, __, ___ = tf.contrib.seq2seq.dynamic_decode(
                        decoder=inference_decoder,
                        maximum_iterations=tf.round(tf.reduce_max(self.encoder_inputs_length)) * 2,
                        impute_finished=False,
                        scope=scope)

                    self.decoder_predictions_inference = tf.identity(self.decoder_outputs_inference.sample_id)
    ```
        
#### Attention Module  
æ³¨æ„åŠ›è®¡ç®—å‘ç”Ÿåœ¨è§£ç æ­¥éª¤ä¸­çš„æ¯ä¸€æ­¥ï¼Œå®ƒåŒ…å«ä¸‹åˆ—æ­¥éª¤ï¼š
1. å½“å‰ç›®æ ‡éšçŠ¶æ€ï¼ˆtarget hidden stateï¼‰å’Œæ‰€æœ‰æºçŠ¶æ€ï¼ˆsource hidden stateï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œä»¥è®¡ç®—æ³¨æ„åŠ›æƒé‡(attention weight)ã€‚
2. åŸºäºæ³¨æ„åŠ›æƒé‡ï¼Œæˆ‘ä»¬è®¡ç®—äº†ä¸€ä¸ªèƒŒæ™¯å‘é‡(context vector)ï¼Œä½œä¸ºæºçŠ¶æ€çš„åŠ æƒå‡å€¼ã€‚
3. å°†èƒŒæ™¯å‘é‡ä¸å½“å‰ç›®æ ‡éšè”½æ€è¿›è¡Œç»“åˆã€‚
    ```py
    if self.attention:
                    attention_states = enc_outputs

                    attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                        self.decoder_num_units,
                        attention_states,
                        memory_sequence_length=enc_lengths)

                    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                        cell,
                        attention_mechanism,
                        attention_layer_size=self.decoder_num_units)

                    if self.beam_search and self.mode == "Infer":
                        initial_state = decoder_cell.zero_state(self.batch_size * self.beam_width, tf.float32)
                    else:
                        initial_state = decoder_cell.zero_state(self.batch_size, tf.float32)

                    initial_state = initial_state.clone(cell_state=dec_start_state)
                else:
                    initial_state = dec_start_state
    ```
    ```py
    # æ³¨æ„åŠ›æƒé‡åˆ¶å›¾å‡½æ•°
    def plot_attention(attention, sentence, predicted_sentence):
        fig = plt.figure(figsize=(10,10))
        ax = fig.add_subplot(1, 1, 1)
        ax.matshow(attention, cmap='viridis')

        fontdict = {'fontsize': 14}

        ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)
        ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)

        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))
        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))

        plt.show()
    ```
### è®­ç»ƒ
1. å¯¹Decoderçš„æ¯ä¸ªæ—¶é—´æ­¥ï¼Œç”±labelå’Œä¸Šè¿°logitå‘é‡ï¼Œè®¡ç®—äº¤å‰ç†µä½œä¸ºæŸå¤±;
2. ç”±äºâ€œ\<pad>â€æ˜¯ç”¨äºå¡«å……çš„äººä¸ºæ·»åŠ ç¬¦å·ï¼Œæ‰€ä»¥ä¸èƒ½å¯¹å…¶è®¡ç®—æŸå¤±ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨"æ©ç "é¿å…å¡«å……é¡¹å¯¹å¥å­æ€»ä½“æŸå¤±å‡½æ•°è®¡ç®—çš„å½±å“ï¼›
3. è®¡ç®—æ¢¯åº¦ï¼Œå¹¶å°†å…¶åº”ç”¨äºä¼˜åŒ–å™¨å’Œåå‘ä¼ æ’­ï¼›
4. è®­ç»ƒå¤šä¸ªepochï¼Œæ ¹æ®åœ¨éªŒè¯é›†çš„è¡¨ç°é€‰æ‹©æœ€ä½³æ¨¡å‹ç”¨ä½œæµ‹è¯•ã€‚
```py
def train():
    step = 0
    g = tf.Graph()
    with g.as_default():
        model = Seq2SeqModel(
            encoder_num_units=256,
            decoder_num_units=256,
            embedding_size=256,
            num_layers=2,
            source_vocab_size=source_vocab_length,
            target_vocab_size = len(target_id2word),
            batch_size=batch_size,
            attention=True,
            beam_search=True,
            beam_width=beam_width,
            mode="Train"
        )

        with tf.Session(config=tf.ConfigProto()) as sess:
            sess.run(tf.global_variables_initializer())
            saver = tf.train.Saver()
            #summary_writer = tf.summary.FileWriter('../log', graph=sess.graph)

            for _epoch in range(1, batches_in_epoch + 1):
                for _batch in range(max_batches):
                    X, y = my_utils.input_generator(
                        train_source_id,
                        train_target_id,
                        batch_size)
                    feed_dict = model.make_train_inputs(
                        x=X,
                        y=y)
                    _, l, train_samples, summary_str,__ = sess.run(
                        [model.train_op,
                         model.loss,
                         model.decoder_predictions_train,
                         model.summary_op,
                         model.decoder_logits_train],
                        feed_dict)
                    #summary_writer.add_summary(summary_str, _epoch * _batch)
                    if step % 50 == 0:
                        bleu_score = cal_bleu(train_samples,y)
                        print('-' * 50)
                        print('-' * 50)
                        print('step {}'.format(step))
                        print('-' * 50)
                        print('minibatch loss: {}'.format(sess.run(model.loss, feed_dict)))
                        print('-'*50)
                        print("Bleu Score: " + str(bleu_score))
                        print('-' * 50)
                        for i in range(5):
                            train_sentence = ''
                            for word in train_samples[i]:
                                train_sentence += target_id2word[word] + ' '
                            print('Predict Sen: ',end="")
                            print(train_sentence)
                        print('-' * 50)
                        for i in range(5):
                            result_sentence = ''
                            for word in y[i]:
                                result_sentence += target_id2word[word] + ' '
                            print('Target Sen: ',end="")
                            print(result_sentence)
                        print('-' * 50)
                        print('\n\n\n')
                    step += 1

                print(_epoch, 'epoch finished')

                if _epoch % epoch_be_saved == 0:
                    saver.save(sess, '../models/' + 'nmt.ckpt', global_step=step)
                    print('model saved.')

            print('finish training')
```

### æµ‹è¯•
1. æµ‹è¯•çš„æ—¶å€™ï¼ŒDecoderå½“å‰æ—¶é—´æ­¥è¾“å…¥ä¸ºä¸Šä¸€æ—¶é—´æ­¥çš„é¢„æµ‹ï¼Œç¬¬ä¸€ä¸ªæ—¶é—´æ­¥è¾“å…¥ä¸ºâ€œ\<eos>â€ã€‚æ¨¡å‹é¢„æµ‹å‡ºç»“æŸç¬¦â€œ\<eos>â€æˆ–è€…åˆ°è¾¾é¢„è®¾çš„é•¿åº¦é™åˆ¶æ—¶åœæ­¢ç”Ÿæˆï¼›
2. é›†æŸæœç´¢Beam search: è´ªå¿ƒæœç´¢åªé€‰æ‹©äº†æ¦‚ç‡æœ€å¤§çš„ä¸€ä¸ª, è€Œé›†æŸæœç´¢åˆ™é€‰æ‹©äº†æ¦‚ç‡æœ€å¤§çš„å‰kä¸ªã€‚è¿™ä¸ªkå€¼ä¹Ÿå«åšé›†æŸå®½åº¦ï¼ˆBeam Widthï¼‰ã€‚
3. è¯„ä¼°: è®¡ç®—é¢„æµ‹å¥å­ä¸ground trueä¹‹é—´çš„BLEUå€¼ï¼›BLEUæ˜¯ä¸€ç§å¯¹ç”Ÿæˆè¯­å¥è¿›è¡Œè¯„ä¼°çš„æŒ‡æ ‡ã€‚å®Œç¾åŒ¹é…çš„å¾—åˆ†ä¸º1.0ï¼Œè€Œå®Œå…¨ä¸åŒ¹é…åˆ™å¾— åˆ†ä¸º0.0ã€‚å¯ä»¥ç”¨æ¥è®¡ç®—å¾…è¯„ä»·è¯‘æ–‡å’Œä¸€ä¸ªæˆ–å¤šä¸ªå‚è€ƒè¯‘æ–‡é—´çš„è·ç¦»ã€‚ä½¿ç”¨NLTKåŒ…å¯ä»¥è®¡ç®—BLEUå€¼ã€‚

```py
def cal_bleu(predict,result):
    predict = predict.tolist()
    predict_list = []
    for i in range(len(predict)):
        try:
            end_ = predict[i].index(0)
        except:
            end_ = len(predict[i]) - 1
        if end_ < len(result[i]) - 1:
            predict_list.append(predict[i][:len(result[i])])
        else:
            predict_list.append(predict[i][:end_])
    total_score = 0
    for i in range(len(predict_list)):
        predict_sen = []
        result_sen = [[]]
        for j in range(len(predict_list[i])):
            predict_sen.append(target_id2word[predict_list[i][j]])

        for j in range(len(result[i])):
            result_sen[0].append(target_id2word[result[i][j]])
        score = sentence_bleu(result_sen,predict_sen)
        total_score += score
    return total_score / len(predict)
```
   

## å®éªŒç»“æœ 
è®­ç»ƒé›†ï¼š  
ä»¥ä¸‹ä¸ºéƒ¨åˆ†é¢„æµ‹ç»“æœï¼š  
```
Â·Â·Â·
step 2300
----------------------------------------------------------------------------------------------------
minibatch loss: 2.5367274284362793
----------------------------------------------------------------------------------------------------
Bleu Score: 0.4156887843934715
predict sentence: <bos> so what are the workers unique for ? <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> 
target sentence: <bos> so what are the europeans waiting for ? <eos> 

predict sentence: <bos> why on gain activities to anticipate the devastating prolong between nato and the eu â€“ such as used policy toward nato under president recent critical has been massive in the right direction ? <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> 
target sentence: <bos> why not start now to overcome the traditional tension between nato and the eu â€“ especially as french policy toward nato under president nicolas sarkozy has been moving in the right direction ? <eos> 

predict sentence: <bos> a compounded mutual work of the secretary general of nato and of the deep of eu foreign policy in the 96 of his mandate care â€™ t require much time and very . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> 
target sentence: <bos> a regular mutual presence of the secretary general of nato and of the head of eu foreign policy in the councils of both organizations doesn â€™ t require much time and effort . <eos> 

predict sentence: <bos> why on dismay gold mainstream at a high political level ( with the stewardship of nato representing in security relief ) â€“ for instance , by mobilizing the us secretary of state and other back of the administration , such as the top secretary or the 96 of the bilateral protection don , to anticipa
Â·Â·Â·
```
æµ‹è¯•é›†ï¼š  
ä»¥ä¸‹ä¸ºéƒ¨åˆ†é¢„æµ‹ç»“æœï¼š   
è¯¦è§docæ–‡ä»¶å¤¹ä¸­çš„[result.txt](result.txt)   
```
Â·Â·Â·  
990  
<bos> strengthening fatalities blow puzzled perhaps perhaps perhaps perhaps perhaps perhaps perhaps perhaps perhaps perhaps perhaps perhaps perhaps perhaps perhaps perhaps perhaps perhaps track track by not by not by by by by by by by   
991  
<bos> but tayyip , us us also also us us us us us us us us us us us us us us us us us us us us us us us us us us us us   
992   
<bos> the , personnel personnel s , , , , , , , , , . . . . . . . . . . . . . . . . . . . . .   
993    
<bos> . , puzzled gun recently an an an an an an an an an an an an an an an an an an years 2008 years 2008 years 2008 years 2008 years 2008 years 2008   
994    
<bos> should americas , us us us us us us us us us us us us us us us us us us us us us us us us us us us us us us us us   
995  
<bos> . , puzzled personnel an an an an an an an an an an an an an an by by not by by by by by by by by by by by by by by   
996  
<bos> there , , us recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently us also also also also also also also also also also also also also also   
997  
<bos> there , years us we we we we we we we we we we we we we we we we we we we we we we we years we years we us years us years   
998  
<bos> the , the us s by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by   
999  
<bos> there micro-story puzzled us years an years , years , years , years , years , years , us years a years a years a years a years a years a years a years a    
1000  
<bos> the , , , , recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently recently   

final sentence bleu score:  7.521496624102009e-232
```
å¯è§ï¼Œæµ‹è¯•é›†é¢„æµ‹æ•ˆæœå¹¶ä¸æ˜¯ååˆ†ç†æƒ³ï¼Œè¿™åº”è¯¥æ˜¯ç”±äºè·‘çš„è½®æ•°å¤ªå°‘ä»¥åŠç½‘ç»œæ¨¡å‹çš„é—®é¢˜ã€‚å¦ä¸€æ–¹é¢ä¹Ÿæ˜¯ç”±äºæ—¶é—´è¾ƒç´§ï¼Œæ²¡æ¥å¾—åŠå¤šåŠ æ£æ‘©ï¼Œå¤šæ¬¡åœ°è·‘æ¥å®ç°æ›´å¥½çš„æ•ˆæœã€‚
## æ€»ç»“ä¸æ€è€ƒ
### é‡åˆ°çš„å›°éš¾åŠè§£å†³æ–¹æ³•
**1.æ–°çŸ¥è¯†çš„å­¦ä¹ é—®é¢˜**  
è¿™æ¬¡çš„å®éªŒéš¾åº¦æŒ‘æˆ˜ä¹Ÿæ¯”è¾ƒå¤§ï¼Œè¾ƒä¹‹ä¸Šæ¬¡å¤§ä½œä¸šåŒæ ·éœ€è¦å­¦ä¹ è®¸å¤šä¹‹å‰æ²¡æœ‰è¿‡çš„æ–°çŸ¥è¯†ã€‚æ¯”å¦‚è¯´ï¼Œæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœºå™¨ç¿»è¯‘ï¼ŒLSTMæ­å»ºç­‰ç­‰ã€‚è€Œè¿™äº›ä¸œè¥¿å¾€å¾€ç›´æ¥ä¸Šç½‘è°·æ­Œéš¾ä»¥è§£å†³é—®é¢˜ï¼Œè¿™å°±è¿«ä½¿æˆ‘å’Œä»¥å¾€å­¦ä¹ ä¸ä¸€æ ·åœ°å»é˜…è¯»å®˜æ–¹æ–‡æ¡£æˆ–è€…é˜…è¯»ç½‘ä¸Šçš„æ•™ç¨‹ï¼Œå°½ç®¡å®˜æ–¹æ–‡æ¡£è¾ƒä¸ºæ™¦æ¶©ï¼Œä¹Ÿæ²¡æœ‰å…·ä½“çš„è§£æï¼Œæ˜¯å—éš¾å•ƒçš„ç¡¬éª¨å¤´ï¼Œä½†æ˜¯è¿˜æ˜¯è€ƒéªŒäº†æˆ‘çš„è€å¿ƒï¼Œé€šè¿‡å’ŒåŒå­¦äº¤æµä»¥åŠå…±åŒå­¦ä¹ ï¼ŒæŸ¥é˜…ç›¸å…³æ–‡çŒ®ï¼Œè§£å†³äº†æ–°çŸ¥è¯†çš„éš¾é¢˜ã€‚ç½‘ä¸Šçš„æ•™ç¨‹ä¹Ÿæ¯”è¾ƒå…³é”®ï¼Œæˆ‘é‡ç‚¹å‚è€ƒäº†[è¿™ä¸ªçš„å®ç°](https://github.com/tensorflow/docs/blob/master/site/zh-cn/tutorials/text/nmt_with_attention.ipynb)ã€‚  
 
**2. TensorFlowä½¿ç”¨é—®é¢˜**  
TensorFlow-GPUçš„è¿è¡Œé€Ÿåº¦ä¼šå¿«ä¸€äº›ï¼Œå¯æƒœçš„æ˜¯åœ¨import tensorflowçš„æ—¶å€™å‡ºç°äº†é—®é¢˜ï¼Œcould not find cuda xxxï¼Œç»æŸ¥é˜…ç›¸å…³é”™è¯¯ï¼Œå‘ç°æ˜¯ç”±äºæ˜¾å¡å¹¶éè‹±ä¼Ÿè¾¾çš„ï¼Œè€Œä½¿ç”¨ä¸äº†TensorFlow-GPUï¼Œåªèƒ½ä½¿ç”¨TensorFlow-CPUäº†ï¼Œå¯¼è‡´åæ¥è®­ç»ƒæ¨¡å‹çš„æ—¶å€™ï¼Œä¸€æ¬¡epochå°±è¦äºŒä¸‰ååˆ†é’Ÿï¼Œå¤§å¤§é™åˆ¶è®­ç»ƒæ¬¡æ•°ã€‚è¿™ä¸ªé—®é¢˜ä¹Ÿå¯¼è‡´äº†æœ€åæ¨¡å‹çš„æ•ˆæœè¾ƒå·®ã€‚   
 
**3. pythonç”¨æ³•é—®é¢˜**  
è¿™ä¸€æ¬¡å®éªŒè®©æˆ‘å¯¹pythonçš„ä¸€äº›å‡½æ•°åº”ç”¨åœ°æ›´åŠ è‡ªå¦‚äº†ã€‚ä¸€äº›åŒå­¦éœ€è¦åå‡ è¡Œçš„ä»£ç æˆ‘å¯ä»¥ä¸€è¡Œå®ç°ï¼Œç›¸æ¯”ä¸Šæ¬¡çš„åˆ†è¯å»å™ªè¿™æ¬¡çš„ä»£ç å†™çš„æ›´åŠ ç®€æ´äº†è¿ç”¨äº†è®¸å¤šä¸ä¸€æ ·çš„åº“å’Œå‡½æ•°ï¼Œæ‰€ä»¥è¯´ï¼Œåœ¨è§‰å¾—è¿™æ ·å®ç°è¾ƒä¸ºå¤æ‚çš„æ—¶å€™ï¼Œå¯ä»¥å…ˆè°·æ­Œä¸€ä¸‹çœ‹çœ‹æœ‰æ²¡æœ‰æ›´æ–¹ä¾¿çš„åº“æˆ–è€…å‡½æ•°å¯ä»¥ä½¿ç”¨ã€‚  
 
### å¿ƒå¾—ä½“ä¼š
&emsp;ç»è¿‡äº†è¿™ä¸€æ¬¡çš„å®éªŒï¼Œå¯¹æœºå™¨ç¿»è¯‘çš„å…·ä½“å®ç°ï¼Œæ¨¡å‹çš„æ„å»ºå’Œå¯¹æ–‡æœ¬çš„å¤„ç†éƒ½æœ‰æ·±å…¥çš„äº†è§£ã€‚å°½ç®¡å®éªŒä¸­å¤šå¤šå°‘å°‘å­˜åœ¨ä¸€äº›ä¸è¶³è¯¸å¦‚ç¡¬ä»¶ä¸Šçš„é™åˆ¶ç­‰ç­‰ï¼Œä¹Ÿæ˜¯å­¦åˆ°äº†è®¸å¤šï¼Œåœ¨ä»¥åçš„å­¦ä¹ ä¸­ä¼šç»§ç»­åŠªåŠ›ï¼Œæ›´ä¸Šä¸€å±‚æ¥¼ã€‚
